{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrective RAG agent using llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-nomic in ./langandllama/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: langchainhub in ./langandllama/lib/python3.9/site-packages (0.1.15)\n",
      "Requirement already satisfied: chromadb in ./langandllama/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: langchain in ./langandllama/lib/python3.9/site-packages (0.1.17)\n",
      "Requirement already satisfied: langgraph in ./langandllama/lib/python3.9/site-packages (0.0.40)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.0.44-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tavily-python in ./langandllama/lib/python3.9/site-packages (0.3.3)\n",
      "Requirement already satisfied: gpt4all in ./langandllama/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: firecrawl-py in ./langandllama/lib/python3.9/site-packages (0.0.6)\n",
      "Requirement already satisfied: langchain_community in ./langandllama/lib/python3.9/site-packages (0.0.36)\n",
      "Requirement already satisfied: tiktoken in ./langandllama/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in ./langandllama/lib/python3.9/site-packages (from langchain-nomic) (0.1.50)\n",
      "Requirement already satisfied: nomic<4.0.0,>=3.0.12 in ./langandllama/lib/python3.9/site-packages (from langchain-nomic) (3.0.27)\n",
      "Requirement already satisfied: requests<3,>=2 in ./langandllama/lib/python3.9/site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./langandllama/lib/python3.9/site-packages (from langchainhub) (2.31.0.20240406)\n",
      "Requirement already satisfied: build>=1.0.3 in ./langandllama/lib/python3.9/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./langandllama/lib/python3.9/site-packages (from chromadb) (2.7.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in ./langandllama/lib/python3.9/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./langandllama/lib/python3.9/site-packages (from chromadb) (0.110.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./langandllama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in ./langandllama/lib/python3.9/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (4.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./langandllama/lib/python3.9/site-packages (from chromadb) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./langandllama/lib/python3.9/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./langandllama/lib/python3.9/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./langandllama/lib/python3.9/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./langandllama/lib/python3.9/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (1.63.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./langandllama/lib/python3.9/site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./langandllama/lib/python3.9/site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in ./langandllama/lib/python3.9/site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./langandllama/lib/python3.9/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./langandllama/lib/python3.9/site-packages (from chromadb) (3.10.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./langandllama/lib/python3.9/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./langandllama/lib/python3.9/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./langandllama/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./langandllama/lib/python3.9/site-packages (from langchain) (0.6.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./langandllama/lib/python3.9/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in ./langandllama/lib/python3.9/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./langandllama/lib/python3.9/site-packages (from langchain) (0.1.53)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./langandllama/lib/python3.9/site-packages (from tiktoken) (2024.4.28)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./langandllama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./langandllama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./langandllama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./langandllama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./langandllama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in ./langandllama/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in ./langandllama/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in ./langandllama/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (7.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in ./langandllama/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./langandllama/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./langandllama/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in ./langandllama/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./langandllama/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./langandllama/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: click in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (8.1.7)\n",
      "Requirement already satisfied: jsonlines in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (4.0.0)\n",
      "Requirement already satisfied: loguru in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (0.7.2)\n",
      "Requirement already satisfied: rich in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (13.7.1)\n",
      "Requirement already satisfied: pandas in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (2.2.2)\n",
      "Requirement already satisfied: pyarrow in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (16.0.0)\n",
      "Requirement already satisfied: pillow in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (10.3.0)\n",
      "Requirement already satisfied: pyjwt in ./langandllama/lib/python3.9/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (2.8.0)\n",
      "Requirement already satisfied: coloredlogs in ./langandllama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./langandllama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./langandllama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in ./langandllama/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.45b0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.2.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./langandllama/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./langandllama/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./langandllama/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./langandllama/lib/python3.9/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in ./langandllama/lib/python3.9/site-packages (from pydantic>=1.9->chromadb) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./langandllama/lib/python3.9/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./langandllama/lib/python3.9/site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./langandllama/lib/python3.9/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./langandllama/lib/python3.9/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: h11>=0.8 in ./langandllama/lib/python3.9/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./langandllama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./langandllama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./langandllama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./langandllama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./langandllama/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./langandllama/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.18.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./langandllama/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./langandllama/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./langandllama/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in ./langandllama/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./langandllama/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./langandllama/lib/python3.9/site-packages (from rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./langandllama/lib/python3.9/site-packages (from rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (2.17.2)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./langandllama/lib/python3.9/site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (4.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./langandllama/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./langandllama/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./langandllama/lib/python3.9/site-packages (from pandas->nomic<4.0.0,>=3.0.12->langchain-nomic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./langandllama/lib/python3.9/site-packages (from pandas->nomic<4.0.0,>=3.0.12->langchain-nomic) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./langandllama/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./langandllama/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./langandllama/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./langandllama/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./langandllama/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Downloading langgraph-0.0.44-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langgraph\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.0.40\n",
      "    Uninstalling langgraph-0.0.40:\n",
      "      Successfully uninstalled langgraph-0.0.40\n",
      "Successfully installed langgraph-0.0.44\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain-nomic langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py langchain_community tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "filecrawl_loader_api_key = os.environ[\"FILECRAWL_LOADER_API_KEY\"]\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"rag_using_llama3 - {unique_id}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be running the project using our local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running on local llama3\n",
    "local_llm = 'llama3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the libraries required to scan through the links provided\n",
    "- Here we are using Filecrawler to scrape the results of our search query as Filecrawler allows us to get clean and structured data from a website in markdown or JSON format\n",
    "- We are using Recursive character splitter to split the documents in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyanshsingh/Desktop/Personal/LangandLlama/langandllama/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "#index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "urls = [\n",
    "    \"https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost\",\n",
    "    \"https://www.ai-jason.com/learning-ai/gpt5-llm\",\n",
    "    \"https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3\",\n",
    "]\n",
    "\n",
    "docs = [FireCrawlLoader(api_key=filecrawl_loader_api_key, url=url, mode=\"scrape\").load() for url in urls]\n",
    "\n",
    "#Retireve data from the docs like page content, metadata and type of document\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "#initialize the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=256, chunk_overlap=0)\n",
    "#Get the split documents in and store all the splits in doc_splits\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "print(len(doc_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metadata of each document and clean the metadata so that it only has values of string, integer, float, boolean and not lists or arrays (in this example: we don't have any of this in our original metadata so clean_metadata looks very familiar to the original one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out complex metadata and ensure proper document formating\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, \"metadata\"):\n",
    "        clean_metadata = {k: v for k ,v in doc.metadata.items() if isinstance(v, (str ,int, float, bool))}\n",
    "        #print(clean_metadata)\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have filtered_documents with clean metadata\n",
    "We will now create embeddings for these chunks and store it in our vectorestore, here we are using ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add to vectorDB: Chroma is used here for the sake of simplicity and ease of use.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = filtered_docs,\n",
    "    collection_name = \"rag_chroma\",\n",
    "    embedding = GPT4AllEmbeddings() #open source embedding by openAI (we could also use text-ada if we have api access)\n",
    ")\n",
    "#intialize our retriver\n",
    "retriver =  vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVER GRADER\n",
    "\n",
    "This is a LLM based grader that will check the relavancy of our retrieved documents based on the question asked.\n",
    "This will output a JSON format answer indicating\n",
    "- Yes (Documents are relevant)\n",
    "- No (Documents are irrelevant)\n",
    "\n",
    "It will score this for each retrieved document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#Initailizing the LLM, output needed in JSON format\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "#PromptTemplate to get that actually grades the documents with the question asked\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n Give a binary score 'ygg\" or 'no' score to indicate whether the document is relevant to the question. In Provide the binary score as a JSON with a single key 'score' and no premable or explaination. <|eot_id|>|start_header_id|>user<|end_header_id|> Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "            Here is the user question: {question} In <|eot_id|>|start_header_id|>assistant<|end_header_id|>\"\"\" ,\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "#Creating: a CHAIN, We will use this chain later\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "#Testing: the chain on one document\n",
    "question = \"how to save llm cost?\"\n",
    "docs = retriver.invoke(question)\n",
    "doc_text = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\":question, \"document\": doc_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER GENERATOR\n",
    "\n",
    "Creating a chain that will generate answer for the question using the context (documents or websearch results)\n",
    "The result will be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generation-- To save LLM cost, you can try the following methods:\n",
      "\n",
      "* Carefully selecting the right models for specific tasks\n",
      "* Optimizing agent memory\n",
      "* Using techniques like LLM Lingua to remove unnecessary tokens and words from input\n",
      "\n",
      "These strategies can help reduce LLM costs without compromising performance.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|>|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise < leot_id|><|start_header_id|>user<| end_\n",
    "_header_id|>\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer: <|eot_id|>|start_header_id|>assistant<|end_header_id|›\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "#Initializing the LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "#post-processing: joins all the context document recieved to one big context and sepearates 2 contexts using the \\n\\n\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#RAG_CHAIN: Chain that will be used to call this answer generator\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#Testing: testing this chain\n",
    "question = \"How to save LLM cost?\"\n",
    "docs = retriver.invoke(question)\n",
    "context = format_docs(docs)\n",
    "#print(\"--Context--\")\n",
    "#print(context)\n",
    "generation = rag_chain.invoke({\"question\": question, \"context\": context})\n",
    "print(\"--Generation--\",generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEB SEARCH\n",
    "Initializing Web_Search tool. We are using Tavily here: Returns search results for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3) #Retrieving only the top 3 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HALLUCINATION GRADER \n",
    "Using LLM to check if the answer is hallucinating/ is grounded to our provided context or no\n",
    "- If Yes: The Answer is Grounded to our context\n",
    "- If No: The Answer is not Grounded to the context\n",
    "\n",
    "Outputs the result in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retriever Grader\n",
    "\n",
    "#Initializing the LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|>|start_header_id|>system<|end_header_id|> You are a grader assessing whether an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. Provide the binary seore as a JSON with a single key 'score' and no preamble or explanation. <leot_id|><|start_header_id|>user<|ed_header_id|>\n",
    "Here are the facts:\n",
    "\\n --- \\n\n",
    "{documents}\n",
    "\\n --- \\n\n",
    "Here is the answer: {generation} <leot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "#Creating a Chain: This will be used to invoke this grader later\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()#chain\n",
    "\n",
    "#Testing if this chain works correctly\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Grader \n",
    "\n",
    "After checking if the answer is hallucinating we will use this grader to check if the generated answer is answering the question.\n",
    "- If yes: The generated answer answers the question\n",
    "- If no: The generated answer doesnot answer the question\n",
    "\n",
    "Outputs the result in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|>|start_header_id|>system<|end_header_id|> You are a grader assessing whether an answer is userful to resolve a question. Give a binary score 'yes' or 'no' score to indicate whether the answer is useful to resolve a question. Provide the binary seore as a JSON with a single key 'score' and no preamble or explanation. <leot_id|><|start_header_id|>user<|ed_header_id|>\n",
    "Here is the answer:\n",
    "\\n --- \\n\n",
    "{generation}\n",
    "\\n --- \\n\n",
    "Here is the question: {question} <leot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "#Creating a Chain: This will be used to invoke this grader later\n",
    "answer_grader = prompt | llm | JsonOutputParser()#chain\n",
    "\n",
    "#Testing if this chain works correctly\n",
    "answer_grader.invoke({\"question\":question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langraph Setup\n",
    "\n",
    "Here we will create a graph using langraph to give agentic behaviour to all the chains that we have created\n",
    "This will be used to define the logic and routing of the query and give the final structure to the Corrective RAG agent that we are building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "#State: This is where you store all the global variables that you will use through all the nodes and edges\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: weather to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search:str\n",
    "    documents: List[str]\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "## Creating Node\n",
    "    # Nodes: These are the tasks that the multi-actor LLM workflow will use\n",
    "\n",
    "#Retrieve node - Job of this node is to get the retirved document based on the question asked\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from the vectorstore\n",
    "\n",
    "    Args:\n",
    "        state(dict): the current graph state\n",
    "\n",
    "    Returns:\n",
    "        state(dict): New key added to state, documents, that contain the retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVING DOCS---\")\n",
    "\n",
    "    #Calling the state function and creating a local copy within the function\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    #invoking the retirever and getting the documents: This gets the filtered documents by default (check code parameters of the retriever above)\n",
    "    documents = retriver.invoke(question)\n",
    "    return {\"documents\": documents, \"question\":question} #This will overwrite the global state variables mentioned in the function above\n",
    "\n",
    "## Document Grader Node - Job of this node is to grade the retrived documents using the documents and question that will be populated most likely by Retrieve node\n",
    "# if not relevant document called then turn on web search FLAG\n",
    "# if relevant document the see if next document is relevant\n",
    "def grade_docs(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant to the question, we will set a flag to run the websearch\n",
    "\n",
    "    Args:\n",
    "        state(dict): the current graph state\n",
    "\n",
    "    Returns:\n",
    "        state(dict): Filtered out irrelevant docs and updated web_search state\n",
    "    \"\"\"\n",
    "    print(\"---CHECK Documents Relevance ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    #if document is relevant then add those document to filtered documents list\n",
    "    filtered_doc = []\n",
    "    #By default web search is a NO\n",
    "    web_search = \"No\"\n",
    "\n",
    "    #Search Each document\n",
    "    for d in documents:\n",
    "        #Invoke the grader chain\n",
    "        score = retrieval_grader.invoke({\"question\":question, \"document\":d.page_content})\n",
    "        #Get the score in str format from the json output recieved {score: 'yes'}\n",
    "        grade = score['score']\n",
    "        \n",
    "        #Document Relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT ---\")\n",
    "            filtered_doc.append(d)\n",
    "        #Document Non Relevant (then activate the websearch flag and move to the next document)\n",
    "        else:\n",
    "            web_search = \"Yes\"\n",
    "            print(\"---GRADE: IRRELEVANT DOC, DO WEB SEARCH FLAG ACTIVATED---\")\n",
    "            continue\n",
    "\n",
    "    #Update the global state with the new filtered doc and with the websearch flag, no need for question but we are putting this just in case\n",
    "    return {\"documents\": filtered_doc, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "#ANSWER GENERATION: This node is to generate answers based on the documents and questions\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state(dict): the current graph state\n",
    "\n",
    "    Returns:\n",
    "        state(dict): New key added to state, generation that contains the generated LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    #Invoke the generator chain\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "#WEB SEARCH CONTEXT GENERATION: This node is responsible to generate context for a question using web that has no releveant documents.\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Run the web search on the question\n",
    "\n",
    "    Args:\n",
    "    \tstate(dict): the current graph state\n",
    "\n",
    "    Returns:\n",
    "    \tstate(dict): Appended web results to the documents\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---DOING WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    #Use the web_search_tool against teh question initailised earlier and store the answer in web_docs\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join(d[\"content\"] for d in web_docs) #Join data of all the content in all the webdocs using \\n as seperator\n",
    "    web_results = Document(page_content=web_results) #Create a new document and set its page contents from the combined data from above\n",
    "\n",
    "    #if documents state variable already has documents then add this newly created doc to it as well\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    #if documents state variable has no documents then add this new document in it\n",
    "    else:\n",
    "        documents = [web_results] #documents is a list of Documents\n",
    "\n",
    "    #Update the global variables\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Conditional Edges - This is the place where logic is applied on what to do when YES, or NO is recieved output an then routes to the correct Node, \n",
    "    #Condtional Edges do not add anything to the state, they are used to create next executable node for the workflow, hence return values are always nodes\n",
    "\n",
    "##DECIDE IF CAN GENERATE OR DO WEB SEARCH\n",
    "#if the answer after grading docs node raise the flag websearch now here we will route it to the websearch node\n",
    "#if the answer after grading docs node did not raise flag websearch, the use the filtered docs generated from that node and send it forward to generate answer\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determine whether to generate and answer of add web search\n",
    "    This Edge is just after we have finished grading the retrieved documents\n",
    "\n",
    "    Args:\n",
    "    \tstate(dict): the current graph state\n",
    "\n",
    "    Returns:\n",
    "        state(dict): Binary Decision of next node to call\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"---CONDITIONAL EDGE: WEBSEARCH v/s GENERATE---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        print(\"---CONDITIONAL DECISION: NO RELEVANT DOCUMENTS FOUND, DO WEB SEARCH\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        print(\"---CONIDITIONAL DECISION: YES RELEVANT DOCUMENT(S) FOUND ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "##AFTER GENERATING ANSWER CHECK IF ANSWE IS HALLUCINATING AND CORRECT\n",
    "\n",
    "#This conditional node will check if the answer is hallucinating or not\n",
    "#IF answer is not hallucinating (Meaning it is grounded), it will check if the answer is correct\n",
    "    #If answer is correct, answer is good and end the workflow\n",
    "    #If answer is incorrect, do web search\n",
    "#If answer is hallucinating it will go to generate node again\n",
    "\n",
    "def hallucination_edge(state):\n",
    "    \"\"\"\n",
    "    Determine whether the generated answer is grounded or not\n",
    "    if the answer is grounded then \n",
    "\n",
    "    Args:\n",
    "    \tstate(dict): the current graph state\n",
    "\n",
    "    Returns:\n",
    "        state(dict): Binary Decision of next node to call\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"---CHECK HALLUCINATION\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    #using our hallucination grader chain\n",
    "    hallucination_score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = hallucination_score['score']\n",
    "\n",
    "    #check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: THE GENERATION IS GROUNDED IN DOCUMENTS\")\n",
    "        print(\"---IS THE ANSWER USEFUL OR NOT? ---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade  = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: YES THE ANSWER IS USEFUL ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: NO THE ANSWER IS NOT USEFUL ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: YES THE GENERATION IS HALLUCINATING IN DOCUMENTS ---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the langgraph worflow using all the nodes and conditional edges created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVING DOCS---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK Documents Relevance ---\n",
      "---GRADE: DOCUMENT RELEVANT ---\n",
      "---GRADE: DOCUMENT RELEVANT ---\n",
      "---GRADE: DOCUMENT RELEVANT ---\n",
      "---GRADE: DOCUMENT RELEVANT ---\n",
      "---CONDITIONAL EDGE: WEBSEARCH v/s GENERATE---\n",
      "---CONIDITIONAL DECISION: YES RELEVANT DOCUMENT(S) FOUND ---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATION\n",
      "---DECISION: THE GENERATION IS GROUNDED IN DOCUMENTS\n",
      "---IS THE ANSWER USEFUL OR NOT? ---\n",
      "---DECISION: YES THE ANSWER IS USEFUL ---\n",
      "'Finished running: generate:'\n",
      "To save Large Language Model (LLM) costs, you can try the following strategies:\n",
      "\n",
      "* Carefully select the right models for specific tasks.\n",
      "* Optimize agent memory to minimize the number of tokens required for each interaction.\n",
      "* Use techniques like LLM Lingua to remove unnecessary tokens and words from the input.\n",
      "\n",
      "These methods can help achieve cost savings while maintaining high performance and user experience.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "#Initialize the graph with all the state variables\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "#Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve) #name, actual node name\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"grade_documents\", grade_docs)\n",
    "\n",
    "#Build Graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\") #connects retrive -> grade_documents\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\", #starting node for the conditional edge\n",
    "    decide_to_generate, # end node based on whatever result comes in\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"websearch\": \"websearch\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\") #direct edge from websearch -> generate\n",
    "workflow.add_conditional_edges( #2 conditions in one originating from the generate edge\n",
    "    \"generate\",\n",
    "    hallucination_edge,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")\n",
    "\n",
    "#Compile the workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "#test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"How to save LLM cost?\"}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "print(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langandllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
